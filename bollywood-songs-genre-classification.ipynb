{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2102201,"sourceType":"datasetVersion","datasetId":1260970},{"sourceId":9763025,"sourceType":"datasetVersion","datasetId":5978839}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ADA Project : Bollywood Songs Genre Classification","metadata":{}},{"cell_type":"markdown","source":"## Yash Gawhale (PES1UG22AM915)","metadata":{}},{"cell_type":"markdown","source":"## Vinay Palled (PES1UG22AM914)","metadata":{}},{"cell_type":"markdown","source":"## K Musadiq Pasha (PES1UG22AM079)","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set paths to dataset folders (adjust to match your Kaggle dataset paths)\ngenres = ['bollypop', 'carnatic', 'ghazal', 'semiclassical', 'sufi']\ndata_dir = '/kaggle/input/indian-music-genre-dataset/genrenew'  # Update with your dataset directory\n\n# Initialize dictionary to store file paths by genre\naudio_files = {genre: [] for genre in genres}\n\n# Collect audio file paths\nfor genre in genres:\n    genre_path = os.path.join(data_dir, genre)\n    audio_files[genre] = [os.path.join(genre_path, file) for file in os.listdir(genre_path) if file.endswith('.mp3')]\n\n# Summary: Number of files per genre\nprint(\"Summary of dataset:\")\nfor genre, files in audio_files.items():\n    print(f\"{genre.capitalize()}: {len(files)} files\")\n\n# Function to plot waveform and spectrogram for a sample file\ndef plot_audio_sample(file_path, genre):\n    # Load audio file\n    y, sr = librosa.load(file_path, sr=None)\n    \n    # Plot waveform\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    librosa.display.waveshow(y, sr=sr)\n    plt.title(f'Waveform - {genre}')\n    plt.xlabel(\"Time (s)\")\n    plt.ylabel(\"Amplitude\")\n\n    # Plot spectrogram\n    plt.subplot(1, 2, 2)\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n    plt.colorbar(format=\"%+2.0f dB\")\n    plt.title(f'Spectrogram - {genre}')\n    plt.xlabel(\"Time (s)\")\n    plt.ylabel(\"Frequency (Hz)\")\n    plt.tight_layout()\n    plt.show()\n\n# Plot waveform and spectrogram for a sample file from each genre\nfor genre, files in audio_files.items():\n    print(f\"Plotting sample audio from genre: {genre}\")\n    plot_audio_sample(files[0], genre)  # Plot the first file from each genre\n\n# Statistical Analysis: Duration of each file in each genre\ndurations = {genre: [] for genre in genres}\n\nfor genre, files in audio_files.items():\n    for file in files:\n        y, sr = librosa.load(file, sr=None)\n        durations[genre].append(librosa.get_duration(y=y, sr=sr))\n\n# Plotting the duration distribution by genre\nplt.figure(figsize=(10, 6))\nfor genre in genres:\n    sns.histplot(durations[genre], kde=True, label=genre, bins=20)\nplt.title(\"Duration Distribution by Genre\")\nplt.xlabel(\"Duration (seconds)\")\nplt.ylabel(\"Count\")\nplt.legend()\nplt.show()\n\n# Display summary statistics for durations\nprint(\"Summary statistics for durations (seconds):\")\nfor genre, duration_list in durations.items():\n    print(f\"{genre.capitalize()} - Mean: {np.mean(duration_list):.2f}, Std Dev: {np.std(duration_list):.2f}, Max: {np.max(duration_list):.2f}, Min: {np.min(duration_list):.2f}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-30T09:59:16.625875Z","iopub.execute_input":"2024-10-30T09:59:16.626758Z","iopub.status.idle":"2024-10-30T10:00:35.049936Z","shell.execute_reply.started":"2024-10-30T09:59:16.626713Z","shell.execute_reply":"2024-10-30T10:00:35.049011Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Audio Data Processing and Visualization\r\n\r\nThis block organizes the dataset, visualizes sample audio files, and analyzes file durations.\r\n\r\n## Key Steps:\r\n1. **Import Libraries**: \r\n   - `librosa` for audio processing.\r\n   - `matplotlib` and `seaborn` for visualizations.\r\n2. **Dataset Setup**: \r\n   - Genres include Bollypop, Carnatic, Ghazal, Semiclassical, and Sufi.\r\n   - Audio file paths are organized by genre into a dictionary.\r\n3. **Dataset Summary**:\r\n   - Total files per genre are printed for an overview.\r\n4. **Audio Visualization**:\r\n   - A `plot_audio_sample` function generates waveforms (amplitude vs. time) and spectrograms (frequency vs. time) for sample files.\r\n5. **Duration Analysis**:\r\n   - Calculates and stores durations of audio files by genre.\r\n   - Plots duration distributions and displays summary statistics (mean, std, min, max).\r\n\r\nThis setup ensures the dataset is well-organized, with initial insights into its structure and characteristics, laying the groundwork for feature extraction and modeling.\r\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport librosa\nimport numpy as np\nimport random\n\n# Set parameters for preprocessing\nSAMPLE_RATE = 16000  # Standard sample rate\nDURATION = 30  # Target duration for each audio file (in seconds)\nAUDIO_LENGTH = SAMPLE_RATE * DURATION  # Number of samples needed for target duration\n\n# Function to preprocess audio files: resampling, trimming/padding, normalization\ndef preprocess_audio(file_path, sample_rate=SAMPLE_RATE, audio_length=AUDIO_LENGTH):\n    # Load audio file\n    y, sr = librosa.load(file_path, sr=sample_rate)\n    \n    # Trim or pad to ensure fixed length\n    if len(y) > audio_length:\n        y = y[:audio_length]\n    else:\n        y = np.pad(y, (0, max(0, audio_length - len(y))), mode='constant')\n    \n    # Normalize audio\n    y = librosa.util.normalize(y)\n    \n    return y\n\n# Optional: Function for data augmentation\ndef augment_audio(y, sr=SAMPLE_RATE):\n    # Apply random pitch shift\n    y_pitch = librosa.effects.pitch_shift(y, sr=sr, n_steps=random.uniform(-2, 2))\n    \n    # Apply random time stretch (avoiding extreme stretching)\n    y_stretch = librosa.effects.time_stretch(y, rate=random.uniform(0.8, 1.2))\n    \n    # Add random noise\n    noise = np.random.normal(0, 0.005, len(y))\n    y_noise = y + noise\n    \n    # Return augmented versions\n    return [y_pitch, y_stretch, y_noise]\n\n\n# Preprocess and optionally augment dataset\npreprocessed_data = {genre: [] for genre in genres}\n\nfor genre, files in audio_files.items():\n    print(f\"Processing genre: {genre}\")\n    for file in files:\n        # Preprocess audio\n        y = preprocess_audio(file)\n        \n        # Store preprocessed audio\n        preprocessed_data[genre].append(y)\n        \n        # Optional: Data augmentation\n        augmented_samples = augment_audio(y)\n        preprocessed_data[genre].extend(augmented_samples)\n\n# Check shape of preprocessed audio data to ensure consistency\nfor genre, audio_list in preprocessed_data.items():\n    print(f\"{genre.capitalize()}: {len(audio_list)} samples, Sample shape: {audio_list[0].shape}\")\n\n# Save preprocessed audio data (optional)\n# Example: Save preprocessed data in .npy format for easy loading later\noutput_dir = '/kaggle/working/preprocessed_audio/'\nos.makedirs(output_dir, exist_ok=True)\nfor genre, audio_list in preprocessed_data.items():\n    genre_dir = os.path.join(output_dir, genre)\n    os.makedirs(genre_dir, exist_ok=True)\n    for idx, audio in enumerate(audio_list):\n        np.save(os.path.join(genre_dir, f'{genre}_{idx}.npy'), audio)\nprint(\"Preprocessed audio data saved.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T10:08:27.627089Z","iopub.execute_input":"2024-10-30T10:08:27.627803Z","iopub.status.idle":"2024-10-30T10:11:58.159920Z","shell.execute_reply.started":"2024-10-30T10:08:27.627763Z","shell.execute_reply":"2024-10-30T10:11:58.158910Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Audio Preprocessing and Augmentation\r\n\r\nThis block preprocesses audio files to prepare them for consistent analysis and applies optional data augmentation to increase data diversity.\r\n\r\n## Key Steps:\r\n1. **Set Parameters**:\r\n   - `SAMPLE_RATE` (16 kHz): Standard sampling rate for consistency.\r\n   - `DURATION` (30 seconds): Target duration for all audio files.\r\n   - `AUDIO_LENGTH`: Total number of samples needed per file.\r\n\r\n2. **Preprocessing Function**:\r\n   - Resamples audio to `SAMPLE_RATE`.\r\n   - Trims or pads files to ensure a fixed length of `AUDIO_LENGTH`.\r\n   - Normalizes audio signals for uniform scaling.\r\n\r\n3. **Data Augmentation**:\r\n   - **Pitch Shift**: Alters pitch randomly within a small range.\r\n   - **Time Stretch**: Speeds up or slows down audio slightly.\r\n   - **Noise Addition**: Adds small random noise for robustness.\r\n\r\n4. **Dataset Preprocessing**:\r\n   - Each file is preprocessed and optionally augmented with pitch-shifted, time-stretched, and noisy versions.\r\n   - Results are stored in a dictionary `preprocessed_data` organized by genre.\r\n\r\n5. **Validation**:\r\n   - Prints the number of processed samples and shape consistency for each genre.\r\n\r\n6. **Optional Save**:\r\n   - Saves preprocessed audio files as `.npy` files for quick access in future analyses.\r\n\r\nThis process ensures uniformity in audio length and scale, while augmentation enhances model robustness by introducing variability in the dataset.\r\n","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport numpy as np\nimport librosa\nimport pandas as pd\nfrom tqdm import tqdm\n\n# Paths to preprocessed data\noutput_dir = '/kaggle/working/preprocessed_audio/'\ngenres = ['bollypop', 'carnatic', 'ghazal', 'semiclassical', 'sufi']\n\n# Initialize lists to store features and labels\nfeatures = []\nlabels = []\n\n# Feature extraction function\ndef extract_features(y, sr):\n    # Extract MFCCs\n    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n    mfccs_mean = np.mean(mfccs, axis=1)\n    \n    # Extract Chroma feature\n    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n    chroma_mean = np.mean(chroma, axis=1)\n    \n    # Spectral features\n    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n    spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n    spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n    \n    # Rhythm feature: Tempo\n    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n    tempo = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)[0]\n    \n    # Combine all features into a single array\n    feature_vector = np.hstack([\n        mfccs_mean,            # 13 MFCC features\n        chroma_mean,           # 12 Chroma features\n        spectral_centroid,     # Spectral centroid\n        spectral_bandwidth,    # Spectral bandwidth\n        spectral_rolloff,      # Spectral rolloff\n        tempo                  # Tempo\n    ])\n    \n    return feature_vector\n\n# Extract features for each genre and save labels\nfor genre in genres:\n    genre_dir = os.path.join(output_dir, genre)\n    for file_name in tqdm(os.listdir(genre_dir), desc=f\"Processing {genre}\"):\n        file_path = os.path.join(genre_dir, file_name)\n        \n        # Load the preprocessed audio sample\n        y = np.load(file_path)\n        \n        # Extract features\n        feature_vector = extract_features(y, sr=SAMPLE_RATE)\n        \n        # Append features and label to lists\n        features.append(feature_vector)\n        labels.append(genre)\n\n# Convert features and labels to a DataFrame\nfeatures_df = pd.DataFrame(features)\nfeatures_df['label'] = labels\n\n# Save features to CSV for easy loading later\nfeatures_csv_path = '/kaggle/working/audio_features.csv'\nfeatures_df.to_csv(features_csv_path, index=False)\nprint(f\"Feature extraction complete. Features saved to {features_csv_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T10:15:56.372483Z","iopub.execute_input":"2024-10-30T10:15:56.373387Z","iopub.status.idle":"2024-10-30T10:26:56.667571Z","shell.execute_reply.started":"2024-10-30T10:15:56.373345Z","shell.execute_reply":"2024-10-30T10:26:56.666646Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Audio Feature Extraction\r\n\r\nThis block focuses on extracting meaningful audio features from preprocessed audio files to prepare the dataset for machine learning models.\r\n\r\n## Key Steps:\r\n1. **Setup**:\r\n   - Specifies the path to preprocessed audio files.\r\n   - Defines genres and initializes lists for storing extracted features and their corresponding labels.\r\n\r\n2. **Feature Extraction Function**:\r\n   - **MFCCs (Mel Frequency Cepstral Coefficients)**: Captures timbral features (13 coefficients).\r\n   - **Chroma Features**: Represents harmonic and pitch content (12 features).\r\n   - **Spectral Features**:\r\n     - Centroid: Indicates brightness of sound.\r\n     - Bandwidth: Measures spread of the spectrum.\r\n     - Rolloff: Frequency below which 85% of the spectral energy lies.\r\n   - **Rhythm Feature (Tempo)**: Captures tempo using onset strength.\r\n   - Combines all extracted features into a single vector.\r\n\r\n3. **Feature Extraction for Dataset**:\r\n   - Iterates through each genre and file in the preprocessed directory.\r\n   - Extracts features for each audio file and appends them to the `features` list.\r\n   - Stores the corresponding genre label in the `labels` list.\r\n\r\n4. **Dataframe Creation**:\r\n   - Converts the extracted features and labels into a `pandas` DataFrame.\r\n   - Adds a `label` column for genre information.\r\n\r\n5. **Save Features**:\r\n   - Saves the feature DataFrame as a `.csv` file for easy loading in subsequent analyses or model training.\r\n\r\n## Output:\r\nThe extracted features are saved in `/kaggle/working/audio_features.csv`, ready for use in training machine learning models. This process ensures a rich set of audio descriptors are captured for each sample, covering timbral, harmonic, and rhythmic aspects.\r\n","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\n\n# Load feature data\nfeatures_csv_path = '/kaggle/working/audio_features.csv'\ndata = pd.read_csv(features_csv_path)\n\n# Separate features and labels\nX = data.drop(columns=['label']).values  # Feature values\ny = data['label'].values                 # Genre labels\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\ny_categorical = to_categorical(y_encoded)  # Convert labels to categorical format for classification\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n\n# Define the neural network model\nmodel = Sequential([\n    Dense(128, input_shape=(X_train.shape[1],), activation='relu'),\n    Dropout(0.3),\n    Dense(64, activation='relu'),\n    Dropout(0.3),\n    Dense(32, activation='relu'),\n    Dense(5, activation='softmax')  # 5 output units for each genre\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)\n\n# Evaluate the model\ntest_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\nprint(f\"Test Accuracy: {test_accuracy:.2f}\")\n\n# Save the trained model\nmodel.save('/kaggle/working/audio_genre_classification_model.h5')\nprint(\"Model training complete and saved as 'audio_genre_classification_model.h5'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T10:31:33.786493Z","iopub.execute_input":"2024-10-30T10:31:33.786916Z","iopub.status.idle":"2024-10-30T10:31:58.661334Z","shell.execute_reply.started":"2024-10-30T10:31:33.786876Z","shell.execute_reply":"2024-10-30T10:31:58.660386Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Neural Network for Audio Genre Classification\r\n\r\nThis block builds, trains, and evaluates a neural network to classify audio samples into genres based on extracted features.\r\n\r\n## Key Steps:\r\n1. **Load Feature Data**:\r\n   - Reads the feature dataset from the `.csv` file.\r\n   - Separates features (`X`) and genre labels (`y`).\r\n\r\n2. **Label Encoding**:\r\n   - Converts genre labels into numerical format using `LabelEncoder`.\r\n   - Applies one-hot encoding (`to_categorical`) for multi-class classification.\r\n\r\n3. **Train-Test Split**:\r\n   - Splits the data into training (80%) and testing (20%) sets for model evaluation.\r\n\r\n4. **Define Neural Network**:\r\n   - A sequential model with:\r\n     - **Dense Layers**: Fully connected layers with 128, 64, and 32 neurons for feature extraction.\r\n     - **Dropout Layers**: Prevent overfitting by randomly deactivating 30% of neurons.\r\n     - **Output Layer**: 5 neurons (one for each genre) with `softmax` activation for multi-class classification.\r\n\r\n5. **Compile the Model**:\r\n   - Optimizer: **Adam** for efficient training.\r\n   - Loss Function: **Categorical Crossentropy** for multi-class classification.\r\n   - Metric: **Accuracy** to track performance.\r\n\r\n6. **Train the Model**:\r\n   - Trains for 30 epochs with a batch size of 32.\r\n   - Splits the training data further for validation.\r\n\r\n7. **Evaluate the Model**:\r\n   - Tests the model on unseen test data and prints the test accuracy.\r\n\r\n8. **Save the Model**:\r\n   - Saves the trained model as `audio_genre_classification_model.h5` for future predictions or deployment.\r\n\r\n## Output:\r\nThe trained model achieves classification accuracy on the test set and is saved for reuse. This step completes the pipeline from feature extraction to genre prediction using a neural network.\r\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, BatchNormalization\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)  # X is loaded from the feature extraction output\nX_scaled = np.expand_dims(X_scaled, axis=2)  # Reshape for Conv1D layer\n\n# Define the CNN model\ncnn_model = Sequential([\n    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_scaled.shape[1], 1)),\n    BatchNormalization(),\n    MaxPooling1D(pool_size=2),\n    Dropout(0.3),\n\n    Conv1D(128, kernel_size=3, activation='relu'),\n    BatchNormalization(),\n    MaxPooling1D(pool_size=2),\n    Dropout(0.3),\n\n    Conv1D(256, kernel_size=3, activation='relu'),\n    BatchNormalization(),\n    MaxPooling1D(pool_size=2),\n    Dropout(0.3),\n\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.4),\n    Dense(64, activation='relu'),\n    Dropout(0.4),\n    Dense(5, activation='softmax')\n])\n\n# Compile the CNN model\ncnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the CNN model\nhistory = cnn_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n\n# Evaluate the CNN model\ntest_loss, test_accuracy = cnn_model.evaluate(X_test, y_test, verbose=0)\nprint(f\"Test Accuracy: {test_accuracy:.2f}\")\n\n# Save the trained model\ncnn_model.save('/kaggle/working/audio_genre_cnn_model.h5')\nprint(\"CNN model training complete and saved as 'audio_genre_cnn_model.h5'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T10:36:13.718341Z","iopub.execute_input":"2024-10-30T10:36:13.719018Z","iopub.status.idle":"2024-10-30T10:36:36.006101Z","shell.execute_reply.started":"2024-10-30T10:36:13.718977Z","shell.execute_reply":"2024-10-30T10:36:36.005036Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Convolutional Neural Network (CNN) for Audio Genre Classification\r\n\r\nThis block implements a Convolutional Neural Network (CNN) to classify audio samples into genres, leveraging spatial features in the data.\r\n\r\n## Key Steps:\r\n\r\n1. **Feature Standardization**:\r\n   - Uses `StandardScaler` to standardize the feature values to have a mean of 0 and a standard deviation of 1.\r\n   - Reshapes the standardized data for compatibility with the 1D convolutional layers by adding an additional dimension.\r\n\r\n2. **CNN Architecture**:\r\n   - **Conv1D Layers**: Extracts local patterns and relationships in the feature space.\r\n   - **BatchNormalization**: Normalizes intermediate outputs to stabilize training and improve convergence.\r\n   - **MaxPooling1D**: Reduces feature dimensionality and captures dominant patterns.\r\n   - **Dropout Layers**: Prevents overfitting by randomly deactivating neurons during training.\r\n   - **Flatten Layer**: Converts multi-dimensional output into a 1D vector for dense layers.\r\n   - **Dense Layers**: Fully connected layers to combine extracted features for classification.\r\n   - **Output Layer**: Uses `softmax` activation with 5 neurons (one for each genre) for multi-class classification.\r\n\r\n3. **Compile the Model**:\r\n   - **Optimizer**: Adam, for efficient weight updates during training.\r\n   - **Loss Function**: Categorical Crossentropy for multi-class classification.\r\n   - **Metric**: Accuracy to monitor classification performance.\r\n\r\n4. **Train the Model**:\r\n   - Trains the model for 50 epochs with a batch size of 32.\r\n   - Uses 20% of the training data as a validation set.\r\n\r\n5. **Evaluate the Model**:\r\n   - Evaluates the CNN on the test set and prints the test accuracy.\r\n\r\n6. **Save the Model**:\r\n   - Saves the trained CNN model as `audio_genre_cnn_model.h5` for future use.\r\n\r\n## Output:\r\nThe trained CNN model leverages convolutional layers to capture intricate patterns in the audio features and achieves a high test accuracy. The saved model can be used for deployment or further analysis.\r\n","metadata":{}},{"cell_type":"code","source":"pip install librosa tensorflow\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T10:39:49.115562Z","iopub.execute_input":"2024-10-30T10:39:49.115987Z","iopub.status.idle":"2024-10-30T10:40:02.711347Z","shell.execute_reply.started":"2024-10-30T10:39:49.115940Z","shell.execute_reply":"2024-10-30T10:40:02.710030Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q -U keras-tuner\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T11:03:25.490663Z","iopub.execute_input":"2024-10-30T11:03:25.491115Z","iopub.status.idle":"2024-10-30T11:03:37.105142Z","shell.execute_reply.started":"2024-10-30T11:03:25.491071Z","shell.execute_reply":"2024-10-30T11:03:37.104020Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\n\n# Load features and labels\nfeatures_df = pd.read_csv('/kaggle/working/audio_features.csv')\nX = features_df.drop(columns=['label']).values\ny = features_df['label'].astype('category').cat.codes.values  # Encode labels\n\n# Scale features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled = np.expand_dims(X_scaled, axis=2)\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\ny_train = to_categorical(y_train, num_classes=5)\ny_test = to_categorical(y_test, num_classes=5)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T11:04:47.918808Z","iopub.execute_input":"2024-10-30T11:04:47.919844Z","iopub.status.idle":"2024-10-30T11:04:47.972786Z","shell.execute_reply.started":"2024-10-30T11:04:47.919771Z","shell.execute_reply":"2024-10-30T11:04:47.971778Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preparation for Model Training\r\n\r\nThis block handles the installation of required libraries and prepares the dataset for training a deep learning model.\r\n\r\n## Key Steps:\r\n\r\n1. **Install Dependencies**:\r\n   - Installs `librosa` for audio processing and `tensorflow` for building and training deep learning models.\r\n   - Installs `keras-tuner` (optional) for hyperparameter optimization if needed in future steps.\r\n\r\n2. **Load Features and Labels**:\r\n   - Reads the extracted features from `audio_features.csv` into a pandas DataFrame.\r\n   - Separates the feature columns (`X`) from the labels (`y`).\r\n   - Encodes genre labels into numerical categories using pandas `.cat.codes`.\r\n\r\n3. **Feature Scaling**:\r\n   - Standardizes features using `StandardScaler` to ensure all feature values are on the same scale (mean = 0, std = 1).\r\n   - Adds an extra dimension to `X_scaled` for compatibility with models like CNNs.\r\n\r\n4. **Split Dataset**:\r\n   - Splits the dataset into training (80%) and testing (20%) sets using `train_test_split`.\r\n   - Converts the labels (`y_train` and `y_test`) into a one-hot encoded format with `to_categorical` to support multi-class classification.\r\n\r\n## Output:\r\nThe dataset is now ready for deep learning:\r\n- **Features (`X_train` and `X_test`)**: Scaled and reshaped.\r\n- **Labels (`y_train` and `y_test`)**: One-hot encoded.\r\nThese processed inputs ensure compatibility with TensorFlow/Keras models.\r\n","metadata":{}},{"cell_type":"code","source":"from keras_tuner import RandomSearch\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n\ndef build_model(hp):\n    model = Sequential()\n    model.add(Conv1D(hp.Int('conv1_filters', min_value=32, max_value=256, step=32),\n                     kernel_size=3, activation='relu', input_shape=(X_scaled.shape[1], 1)))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Dropout(hp.Float('dropout1', 0.2, 0.5, step=0.1)))\n\n    # Additional layers with variable parameters\n    for i in range(hp.Int('n_layers', 1, 3)):\n        model.add(Conv1D(hp.Int(f'conv{i+2}_filters', min_value=64, max_value=256, step=32),\n                         kernel_size=3, activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Dropout(hp.Float(f'dropout{i+2}', 0.2, 0.5, step=0.1)))\n\n    model.add(Flatten())\n    model.add(Dense(hp.Int('dense_units', min_value=64, max_value=512, step=64), activation='relu'))\n    model.add(Dropout(hp.Float('dense_dropout', 0.2, 0.5, step=0.1)))\n    model.add(Dense(5, activation='softmax'))\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Run Random Search for tuning\ntuner = RandomSearch(\n    build_model,\n    objective='val_accuracy',\n    max_trials=5,\n    executions_per_trial=2,\n    directory='/kaggle/working/tuner',\n    project_name='indian_song_genre_classification'\n)\n\ntuner.search(X_train, y_train, epochs=20, validation_split=0.2, batch_size=32)\nbest_model = tuner.get_best_models(num_models=1)[0]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T11:05:12.295410Z","iopub.execute_input":"2024-10-30T11:05:12.296156Z","iopub.status.idle":"2024-10-30T11:06:23.713608Z","shell.execute_reply.started":"2024-10-30T11:05:12.296117Z","shell.execute_reply":"2024-10-30T11:06:23.712620Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the best model\nhistory = best_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T11:08:01.644118Z","iopub.execute_input":"2024-10-30T11:08:01.644508Z","iopub.status.idle":"2024-10-30T11:08:14.960303Z","shell.execute_reply.started":"2024-10-30T11:08:01.644470Z","shell.execute_reply":"2024-10-30T11:08:14.959555Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameter Tuning for CNN Model\r\n\r\nThis block uses `Keras Tuner` to optimize the hyperparameters of a Convolutional Neural Network (CNN) for audio genre classification.\r\n\r\n## Key Steps:\r\n\r\n1. **Model Building Function**:\r\n   - Defines a CNN architecture with tunable hyperparameters:\r\n     - **Convolutional Layers**:\r\n       - Number of filters (`conv1_filters`, `conv2_filters`, etc.) varies between 32 and 256 in steps of 32.\r\n       - Supports up to 3 layers, determined by the `n_layers` parameter.\r\n     - **Dropout Rate**:\r\n       - Adjustable for each layer between 0.2 and 0.5.\r\n     - **Dense Layer**:\r\n       - Number of neurons (`dense_units`) ranges from 64 to 512 in steps of 64.\r\n       - Includes dropout (`dense_dropout`) for regularization.\r\n\r\n2. **Hyperparameter Tuning**:\r\n   - `RandomSearch` is used to explore different combinations of hyperparameters.\r\n   - Key settings:\r\n     - **Objective**: Maximizing validation accuracy (`val_accuracy`).\r\n     - **Trials**: Up to 5 combinations of hyperparameters, with 2 executions per trial for consistency.\r\n     - **Search Space**: Configured through `build_model`.\r\n\r\n3. **Tuning Execution**:\r\n   - Conducts training on `X_train` with a validation split of 20%.\r\n   - Searches for the optimal combination of hyperparameters.\r\n\r\n4. **Best Model Selection and Training**:\r\n   - Extracts the best-performing model (`best_model`) based on validation accuracy.\r\n   - Retrains the selected model for 50 epochs to fully leverage the best hyperparameter configuration.\r\n\r\n## Output:\r\nThe optimal CNN model is identified and trained with the best hyperparameters for genre classification. This approach ensures a well-optimized architecture for high classification accuracy.\r\n","metadata":{}},{"cell_type":"code","source":"# Evaluate model on test data\ntest_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\nprint(f\"Test Accuracy: {test_accuracy:.2f}\")\n\n# Save model for later use\nbest_model.save('/kaggle/working/best_audio_genre_model.h5')\nprint(\"Model saved as 'best_audio_genre_model.h5'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T11:08:44.957840Z","iopub.execute_input":"2024-10-30T11:08:44.958233Z","iopub.status.idle":"2024-10-30T11:08:45.807132Z","shell.execute_reply.started":"2024-10-30T11:08:44.958188Z","shell.execute_reply":"2024-10-30T11:08:45.806151Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import librosa\n\n# Function for predicting new samples\ndef predict_genre(audio_path):\n    # Preprocess new audio file\n    y, sr = librosa.load(audio_path, sr=16000)\n    y = np.pad(y, (0, max(0, 16000*30 - len(y))), mode='constant')\n    y = librosa.util.normalize(y)\n    \n    # Extract features and scale\n    feature_vector = extract_features(y, sr)  # Reuse `extract_features` function from before\n    feature_vector = scaler.transform([feature_vector])\n    feature_vector = np.expand_dims(feature_vector, axis=2)\n    \n    # Predict\n    prediction = best_model.predict(feature_vector)\n    genre_idx = np.argmax(prediction)\n    genre_labels = ['bollypop', 'carnatic', 'ghazal', 'semiclassical', 'sufi']\n    \n    return genre_labels[genre_idx], prediction[0]\n\n# Example usage\npredicted_genre, prediction_scores = predict_genre('/kaggle/input/testdata/Dunki_ O Maahi.mp3')\nprint(f\"Predicted Genre: {predicted_genre}, Scores: {prediction_scores}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T11:28:55.204613Z","iopub.execute_input":"2024-10-30T11:28:55.205386Z","iopub.status.idle":"2024-10-30T11:28:58.574273Z","shell.execute_reply.started":"2024-10-30T11:28:55.205339Z","shell.execute_reply":"2024-10-30T11:28:58.573062Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Evaluation and Genre Prediction\r\n\r\nThis block evaluates the trained model on test data and demonstrates its usage for predicting the genre of new audio samples.\r\n\r\n## Key Steps:\r\n\r\n### 1. Evaluate the Model\r\n- **Test Set Evaluation**:\r\n  - The best model obtained from hyperparameter tuning is evaluated on the test set.\r\n  - Prints the test accuracy, providing an objective measure of the model's performance on unseen data.\r\n\r\n### 2. Save the Model\r\n- The trained model is saved as `best_audio_genre_model.h5` for future predictions or deployment.\r\n\r\n### 3. Predict New Samples\r\n- **Genre Prediction Function**:\r\n  - Preprocesses the input audio:\r\n    - Resamples it to 16 kHz.\r\n    - Pads or trims it to a fixed length of 30 seconds.\r\n    - Normalizes the audio signal.\r\n  - Extracts features using the previously defined `extract_features` function.\r\n  - Scales the features using the trained `StandardScaler`.\r\n  - Makes predictions using the trained model (`best_model`).\r\n  - Returns the predicted genre and confidence scores for all classes.\r\n\r\n### 4. Example Prediction\r\n- Uses a sample audio file (`Dunki_O_Maahi.mp3`) to test the prediction pipeline.\r\n- Outputs the predicted genre and associated confidence scores.\r\n\r\n## Output:\r\n- **Model Performance**: Displays the test accuracy on unseen data.\r\n- **Saved Model**: The trained model is stored as `best_audio_genre_model.h5`.\r\n- **Prediction Example**: Demonstrates how to predict the genre of a new audio file, showcasing the model's real-world application capabilities.\r\n","metadata":{}},{"cell_type":"code","source":"predicted_genre, prediction_scores = predict_genre('/kaggle/input/testdata/Kahan se aaye badra.mp3')\nprint(f\"Predicted Genre: {predicted_genre}, Scores: {prediction_scores}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-30T11:29:30.714117Z","iopub.execute_input":"2024-10-30T11:29:30.714806Z","iopub.status.idle":"2024-10-30T11:29:35.623291Z","shell.execute_reply.started":"2024-10-30T11:29:30.714762Z","shell.execute_reply":"2024-10-30T11:29:35.622303Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predicted_genre, prediction_scores = predict_genre('/kaggle/input/testdata/kun faya kun.mp3')\nprint(f\"Predicted Genre: {predicted_genre}, Scores: {prediction_scores}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-30T11:30:09.622276Z","iopub.execute_input":"2024-10-30T11:30:09.622679Z","iopub.status.idle":"2024-10-30T11:30:13.916216Z","shell.execute_reply.started":"2024-10-30T11:30:09.622640Z","shell.execute_reply":"2024-10-30T11:30:13.915255Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# Define genre labels\ngenre_labels = ['bollypop', 'carnatic', 'ghazal', 'semiclassical', 'sufi']\n\n# Confusion matrix\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\ny_pred = best_model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\ncm = confusion_matrix(y_true, y_pred_classes)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=genre_labels, yticklabels=genre_labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_true, y_pred_classes, target_names=genre_labels))\n\n\n# Plot training & validation accuracy and loss\nplt.figure(figsize=(12, 4))\n\n# Accuracy\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model Accuracy')\nplt.legend()\n\n# Loss\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Model Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Confusion matrix\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\ny_pred = best_model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\ncm = confusion_matrix(y_true, y_pred_classes)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=genre_labels, yticklabels=genre_labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n\n# Classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_true, y_pred_classes, target_names=genre_labels))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T11:32:00.814244Z","iopub.execute_input":"2024-10-30T11:32:00.814879Z","iopub.status.idle":"2024-10-30T11:32:02.113564Z","shell.execute_reply.started":"2024-10-30T11:32:00.814837Z","shell.execute_reply":"2024-10-30T11:32:02.112644Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Evaluation Metrics and Performance Visualization\r\n\r\nThis block provides a detailed evaluation of the trained model's performance, using a confusion matrix and classification metrics, and visualizes the training process.\r\n\r\n## Key Steps:\r\n\r\n### 1. Confusion Matrix\r\n- **Purpose**: Shows the relationship between true labels and predictions, providing insight into how well the model distinguishes between genres.\r\n- **Implementation**:\r\n  - Computes the confusion matrix using `confusion_matrix` from `sklearn`.\r\n  - Visualizes it as a heatmap with `seaborn`.\r\n  - Includes genre labels on the axes for easy interpretation.\r\n\r\n### 2. Classification Report\r\n- **Purpose**: Summarizes precision, recall, F1-score, and support for each genre, providing a comprehensive evaluation of model performance.\r\n- **Implementation**:\r\n  - Generates the classification report with `classification_report` from `sklearn`.\r\n  - Targets are labeled with genre names for clarity.\r\n\r\n### 3. Training and Validation Metrics Visualization\r\n- **Accuracy**:\r\n  - Plots training and validation accuracy across epochs to monitor how well the model learns over time.\r\n- **Loss**:\r\n  - Plots training and validation loss to assess the model's convergence and detect potential overfitting.\r\n\r\n### Output:\r\n1. **Confusion Matrix**:\r\n   - A heatmap showing prediction accuracy for each genre.\r\n   - Misclassified samples are highlighted for further analysis.\r\n2. **Classification Report**:\r\n   - Metrics for precision, recall, and F1-score for each genre, along with overall accuracy.\r\n3. **Training Curves**:\r\n   - Accuracy and loss curves to visualize model performance during training and validation phases.\r\n\r\nThese visualizations and metrics offer a clear picture of the model's strengths and areas for improvement, enabling deeper insights into its performance.\r\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}